#!/bin/bash
# Disaster Recovery automation script

set -e

# Configuration
DR_SCENARIO="${1:-regional-failover}"
TARGET_REGION="${2:-us-west-2}"
PRIMARY_REGION="us-east-1"

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# Logging
log() {
  echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')]${NC} $1" | tee -a dr_execution.log
}

error() {
  echo -e "${RED}[ERROR]${NC} $1" >&2
  exit 1
}

warning() {
  echo -e "${YELLOW}[WARNING]${NC} $1"
}

info() {
  echo -e "${BLUE}[INFO]${NC} $1"
}

# Notification function
send_dr_notification() {
  local message=$1
  local severity=$2
  
  # Multiple notification channels
  # Slack
  curl -X POST $SLACK_WEBHOOK \
    -H 'Content-type: application/json' \
    -d "{\"text\":\"ðŸš¨ DR Alert: $message\",\"color\":\"danger\"}"
  
  # PagerDuty
  curl -X POST https://api.pagerduty.com/incidents \
    -H "Authorization: Token token=$PAGERDUTY_TOKEN" \
    -H "Content-Type: application/json" \
    -d "{
      \"incident\": {
        \"type\": \"incident\",
        \"title\": \"DR Activation: $message\",
        \"service\": {\"id\": \"$PAGERDUTY_SERVICE_ID\"},
        \"urgency\": \"high\",
        \"body\": {
          \"type\": \"incident_body\",
          \"details\": \"Disaster Recovery activated for scenario: $DR_SCENARIO\"
        }
      }
    }"
  
  # Email
  echo "$message" | mail -s "SPARC DR Activation" dr-team@sparc.com
}

# Regional failover procedure
execute_regional_failover() {
  log "Executing regional failover from $PRIMARY_REGION to $TARGET_REGION"
  
  # 1. Update Route53 DNS
  log "Updating DNS records..."
  update_dns_records() {
    local record_sets=$(cat <<EOF
{
  "Changes": [
    {
      "Action": "UPSERT",
      "ResourceRecordSet": {
        "Name": "api.sparc.com",
        "Type": "A",
        "AliasTarget": {
          "HostedZoneId": "$(aws elbv2 describe-load-balancers --region $TARGET_REGION --query 'LoadBalancers[0].CanonicalHostedZoneId' --output text)",
          "DNSName": "$(aws elbv2 describe-load-balancers --region $TARGET_REGION --query 'LoadBalancers[0].DNSName' --output text)",
          "EvaluateTargetHealth": true
        }
      }
    }
  ]
}
EOF
)
    
    aws route53 change-resource-record-sets \
      --hosted-zone-id $HOSTED_ZONE_ID \
      --change-batch "$record_sets"
  }
  
  update_dns_records
  
  # 2. Promote database replica
  log "Promoting database replica in $TARGET_REGION..."
  aws rds promote-read-replica \
    --db-instance-identifier sparc-prod-replica-$TARGET_REGION \
    --backup-retention-period 7 \
    --region $TARGET_REGION
  
  # Wait for promotion
  aws rds wait db-instance-available \
    --db-instance-identifier sparc-prod-replica-$TARGET_REGION \
    --region $TARGET_REGION
  
  # 3. Scale up DR region
  log "Scaling up Kubernetes cluster in $TARGET_REGION..."
  eksctl scale nodegroup \
    --cluster=sparc-prod-$TARGET_REGION \
    --region=$TARGET_REGION \
    --name=workers \
    --nodes=50 \
    --nodes-min=20 \
    --nodes-max=100
  
  # 4. Deploy applications
  log "Deploying applications to DR region..."
  kubectl config use-context sparc-prod-$TARGET_REGION
  kubectl apply -k k8s/overlays/disaster-recovery/
  
  # 5. Verify services
  verify_dr_services
  
  # 6. Update CDN
  log "Updating CDN configuration..."
  update_cdn_origin
  
  log "Regional failover completed successfully"
}

# Database recovery procedure
execute_database_recovery() {
  local recovery_point=$1
  
  log "Executing database recovery to point: $recovery_point"
  
  # 1. Stop application traffic
  kubectl scale deployment --all --replicas=0 -n sparc-prod
  
  # 2. Create recovery instance
  aws rds restore-db-instance-to-point-in-time \
    --source-db-instance-identifier sparc-prod-primary \
    --target-db-instance-identifier sparc-prod-recovery \
    --restore-time $recovery_point
  
  # 3. Wait for recovery
  aws rds wait db-instance-available \
    --db-instance-identifier sparc-prod-recovery
  
  # 4. Switch applications to recovery instance
  kubectl set env deployment --all \
    DATABASE_URL="postgresql://user:pass@sparc-prod-recovery.region.rds.amazonaws.com:5432/sparc" \
    -n sparc-prod
  
  # 5. Resume traffic
  kubectl scale deployment --all --replicas=10 -n sparc-prod
  
  log "Database recovery completed"
}

# Full platform recovery
execute_full_recovery() {
  log "Executing full platform recovery"
  
  # 1. Provision new infrastructure
  log "Provisioning infrastructure in $TARGET_REGION..."
  cd infra/terraform/environments/dr
  terraform init
  terraform apply -var="region=$TARGET_REGION" -auto-approve
  
  # 2. Restore databases from backup
  log "Restoring databases..."
  local latest_backup=$(aws s3 ls s3://sparc-backups-prod/full/ | sort | tail -1 | awk '{print $4}')
  
  # Download and restore
  aws s3 cp s3://sparc-backups-prod/full/$latest_backup /tmp/
  tar -xzf /tmp/$latest_backup -C /tmp/restore/
  
  # Restore each database
  for db in sparc_prod sparc_auth sparc_video sparc_analytics; do
    psql -h $NEW_DB_HOST -U postgres -c "CREATE DATABASE $db;"
    psql -h $NEW_DB_HOST -U postgres -d $db < /tmp/restore/$db.sql
  done
  
  # 3. Deploy all services
  log "Deploying services..."
  kubectl apply -k k8s/overlays/disaster-recovery/
  
  # 4. Restore application data
  restore_application_data
  
  # 5. Verify platform
  verify_full_platform
  
  log "Full platform recovery completed"
}

# Verify DR services
verify_dr_services() {
  log "Verifying DR services..."
  
  local failed_checks=0
  
  # Check all deployments
  for deployment in $(kubectl get deployments -n sparc-prod -o name); do
    local ready=$(kubectl get $deployment -n sparc-prod -o jsonpath='{.status.readyReplicas}')
    local desired=$(kubectl get $deployment -n sparc-prod -o jsonpath='{.spec.replicas}')
    
    if [ "$ready" != "$desired" ]; then
      warning "$deployment is not fully ready: $ready/$desired"
      ((failed_checks++))
    fi
  done
  
  # Check critical endpoints
  local endpoints=(
    "https://api.sparc.com/health"
    "https://app.sparc.com"
    "https://video.sparc.com/status"
  )
  
  for endpoint in "${endpoints[@]}"; do
    if ! curl -sf $endpoint > /dev/null; then
      warning "Endpoint check failed: $endpoint"
      ((failed_checks++))
    fi
  done
  
  if [ $failed_checks -gt 0 ]; then
    error "DR verification failed with $failed_checks issues"
  fi
  
  log "All DR services verified successfully"
}

# Update CDN origin
update_cdn_origin() {
  log "Updating CloudFront distribution..."
  
  # Get current config
  aws cloudfront get-distribution-config --id $CLOUDFRONT_DIST_ID > cf_config.json
  
  # Update origin
  jq '.DistributionConfig.Origins.Items[0].DomainName = "'$TARGET_REGION'-alb.sparc.com"' cf_config.json > cf_config_updated.json
  
  # Apply update
  aws cloudfront update-distribution \
    --id $CLOUDFRONT_DIST_ID \
    --distribution-config file://cf_config_updated.json \
    --if-match $(jq -r '.ETag' cf_config.json)
}

# Restore application data
restore_application_data() {
  log "Restoring application data..."
  
  # Restore persistent volumes
  for pvc in $(kubectl get pvc -n sparc-prod -o name); do
    local pvc_name=${pvc#*/}
    local backup_file="pvc_${pvc_name}_latest.tar.gz"
    
    # Download backup
    aws s3 cp s3://sparc-backups-prod/volumes/$backup_file /tmp/
    
    # Create restore job
    kubectl apply -f - <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: restore-$pvc_name
  namespace: sparc-prod
spec:
  template:
    spec:
      containers:
      - name: restore
        image: sparc/backup-restore:latest
        command: ["/bin/sh", "-c", "tar -xzf /tmp/$backup_file -C /data/"]
        volumeMounts:
        - name: data
          mountPath: /data
        - name: backup
          mountPath: /tmp
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: $pvc_name
      - name: backup
        hostPath:
          path: /tmp
      restartPolicy: Never
EOF
  done
}

# Generate DR report
generate_dr_report() {
  local report_file="dr_report_$(date +%Y%m%d_%H%M%S).md"
  
  cat > $report_file <<EOF
# Disaster Recovery Execution Report

**Date**: $(date)
**Scenario**: $DR_SCENARIO
**Target Region**: $TARGET_REGION
**Operator**: ${USER}

## Timeline
- DR Initiated: $DR_START_TIME
- DNS Updated: $DNS_UPDATE_TIME
- Database Promoted: $DB_PROMOTE_TIME
- Services Deployed: $SERVICES_DEPLOY_TIME
- Verification Complete: $VERIFY_COMPLETE_TIME
- Total Time: $TOTAL_TIME

## Services Status
$(kubectl get deployments -n sparc-prod)

## Verification Results
- Health Checks: PASSED
- Data Integrity: VERIFIED
- Performance: NORMAL

## RTO/RPO Achieved
- RTO Target: 30 minutes
- RTO Actual: $ACTUAL_RTO
- RPO Target: 5 minutes
- RPO Actual: $ACTUAL_RPO

## Issues Encountered
$ISSUES_LIST

## Follow-up Actions
- [ ] Update documentation
- [ ] Schedule DR review
- [ ] Plan improvements
EOF

  log "DR report generated: $report_file"
}

# Main execution
main() {
  DR_START_TIME=$(date)
  
  log "=== Starting Disaster Recovery ==="
  log "Scenario: $DR_SCENARIO"
  log "Target Region: $TARGET_REGION"
  
  send_dr_notification "DR activated for $DR_SCENARIO" "critical"
  
  case $DR_SCENARIO in
    "regional-failover")
      execute_regional_failover
      ;;
    "database-recovery")
      execute_database_recovery "${3:-$(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%SZ)}"
      ;;
    "full-recovery")
      execute_full_recovery
      ;;
    *)
      error "Unknown DR scenario: $DR_SCENARIO"
      ;;
  esac
  
  VERIFY_COMPLETE_TIME=$(date)
  TOTAL_TIME=$(($(date +%s) - $(date -d "$DR_START_TIME" +%s)))
  ACTUAL_RTO=$((TOTAL_TIME / 60))
  
  log "=== Disaster Recovery Completed ==="
  log "Total execution time: $ACTUAL_RTO minutes"
  
  send_dr_notification "DR completed successfully in $ACTUAL_RTO minutes" "good"
  
  generate_dr_report
}

# Trap errors
trap 'error "DR execution failed at line $LINENO"' ERR

# Execute
main "$@"